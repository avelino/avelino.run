---
date: "2025-11-16"
draft: false
title: "Developer Resistance to AI in Programming: Psychology and Data"
tags: ["AI", "LLMs", "developer psychology", "coding assistants", "AI adoption", "developer resistance", "productivity", "software development", "programming"]
description: "Exploring why developers resist AI coding tools through recent data on adoption rates and psychological factors, while discussing how AI can be a powerful ally when used with discernment and business context understanding"
url: "/developer-resistance-ai-programming-psychology-data"
---

In recent years, AI tools based on LLMs (large language models) have become widely available to assist with programming. Platforms like GitHub Copilot and ChatGPT promise to autocomplete code, suggest solutions, and boost developer productivity. Even with all this potential, many developers show resistance or hesitation when it comes to adopting these tools in their daily work.

This blog post explores **why** that happens, bringing together **recent data** on AI adoption and **psychological aspects** that help explain this reluctance. We'll also discuss how AI can be a powerful ally for developers – as long as it's used with discernment and a solid understanding of the business context (which only humans actually have).

## The Rise of AI Coding Tools

The adoption of AI coding assistants has grown rapidly. According to a Google DORA report (2025), **90% of technology professionals already use AI in their workflows**, a jump of 14% compared to the previous year ([source: observer.com](https://observer.com)). These assistants are used for tasks ranging from writing code snippets to executing tests and performing security reviews ([observer.com](https://observer.com)). Leading companies are investing heavily in this space – and in fact, **85% of developers say AI has made them more productive**, although 41% say the improvement was only "slight" ([observer.com](https://observer.com)).

Despite the initial optimism, it's not all sunshine and rainbows. **Trust in AI tools has not grown at the same pace as adoption.** Only 24% of developers say they trust AI-generated code "a lot," while almost a third admit they trust it "very little" or not at all ([observer.com](https://observer.com)). Other surveys confirm this caution: more developers **distrust** AI accuracy (46%) than **trust** it (33%), and only a small fraction (~3%) report full confidence in the assistant's responses ([survey.stackoverflow.co](https://survey.stackoverflow.co)).

In other words, AI has entered the workflow, but **with one foot out the door**. Many developers see it "as an assistant, not a partner," as a Google Cloud research lead put it ([observer.com](https://observer.com)). This paradox – **high availability of AI versus low trust** – sets the stage for understanding where the resistance comes from.

## Resistance to Change Is Real

It's no surprise there's hesitation. Resisting change in the workplace is a very common human reaction, especially when it involves disruptive innovation. With the new wave of AI tools, this shows up as **lower-than-expected adoption** and **shy usage of more advanced features**.

For example, a large study with **28,698 software engineers** showed that **only 41%** used an AI coding assistant in the first year after it was introduced, even with organizational encouragement ([papers.ssrn.com](https://papers.ssrn.com)). In certain groups, adoption was even lower – only **31% of women engineers** and **39% of developers over 40** used the tool at all ([papers.ssrn.com](https://papers.ssrn.com)). These numbers illustrate how strong the **cultural and psychological barrier** to adoption can be, even when the technology is already there.

From an organizational standpoint, **three out of four companies report that the hardest part of implementing AI is getting people to change how they work** ([bain.com](https://bain.com)). Under pressure or tight deadlines, many developers "fall back to old habits" instead of using the new tool ([bain.com](https://bain.com)). This resistance doesn't mean stubbornness for no reason – there are usually **legitimate concerns** underneath it.

Next, we'll look at the main **psychological factors** that make developers wary of relying on AI.

## Why Are Developers Afraid? (Psychological Factors)

### Fear of Losing Their Job or Relevance

A latent concern is that AI might make developers obsolete. **Almost half of professionals (49%) fear that automation will replace their role in the next five years** ([about.gitlab.com](https://about.gitlab.com)). This fear of substitution can drive resistance: some people avoid adopting the tool so they don't "train their own replacement."

On top of that, some worry that using AI could **undermine their importance on the team** – after all, if "anyone with AI" can produce code, where does that leave the value of the human specialist? Research points exactly to this: some engineers are concerned that AI "hurts their role" within the team ([bain.com](https://bain.com)). This sense of vulnerability in their job is a powerful reason to keep AI at arm's length.

### Fear of Appearing Less Competent

Developers tend to be proud of their ability to solve complex problems. Admitting they need help from an AI can sound, to some, like a sign of lack of competence.

A recent academic study describes this as a **"competence penalty"**: in an experiment, engineers who used AI to help write code received **competence ratings 9% lower** (for the exact same output) compared to those who did not use AI – and women were penalized even more heavily ([papers.ssrn.com](https://papers.ssrn.com)).

In other words, there is a perception (and sometimes a reality) that those who rely on AI are seen as less capable. Many developers **anticipate this stigma** and therefore avoid using the tool to protect their professional reputation ([papers.ssrn.com](https://papers.ssrn.com)). Put simply, **ego and identity** are on the line: *"If I don't write every line myself, am I less of a developer?"* That mindset can drag adoption down, especially in cultures that glorify the lone hero who codes everything by hand.

### Distrust in the Accuracy and Quality of AI-Generated Code

Programmers are naturally skeptical – finding errors and bugs is literally part of the job. With AI, it's no different. There is a **trust deficit** when it comes to AI-generated answers.

As mentioned earlier, the majority do not fully trust AI-suggested code (only ~3% trust it blindly) ([survey.stackoverflow.co](https://survey.stackoverflow.co)). More experienced developers are the most cautious group, with the highest rate of "no trust at all" (20%) among experience levels ([survey.stackoverflow.co](https://survey.stackoverflow.co)).

This caution is justified: AI models often provide solutions that are **"almost correct, but not quite"**, which can introduce subtle errors and require extra debugging later ([arstechnica.com](https://arstechnica.com)). Many developers report frustration when AI hallucinates incorrect code or provides context-free suggestions – for example, a generic solution that simply doesn't apply to the specific case at hand.

Each failure reduces trust and reinforces the idea that it's safer to rely on your own knowledge. If you're going to review every line of AI-generated code in detail, **some developers prefer to write everything from scratch**.

### Attachment to Control and Comfort Zone

Software development is not just a technical activity – it's also a craft of **authorship and control**. Handing part of that control over to a machine can feel uncomfortable.

Many developers are proud of understanding **every line of code they write**; with AI generating entire blocks, there is a real fear of becoming a passive operator. Unsurprisingly, most people see AI as "just an assistant that suggests things," and hesitate to "hand over the wheel" completely ([observer.com](https://observer.com)).

There's also the habit factor: years of experience shape personal workflows. Under pressure, professionals usually choose what they already know – even if the new tool might theoretically be better ([bain.com](https://bain.com)). Learning to use AI effectively (writing good prompts, interpreting and debugging its output) takes time and a mindset shift. Without proper training, the novelty can feel more like an obstacle than a help. In fact, companies note that **AI skill gaps** (for example, knowing how to review AI-generated code) contribute to AI being used **far below its potential** ([bain.com](https://bain.com)).

In short, **changing habits is hard** – and in everyday work, many people prefer to stick with familiar tools and methods, where they feel in control.

## AI as an Ally: Balancing Productivity and Caution

Despite all the resistance, it's important to highlight that those who manage to **integrate AI into their work** see **real, tangible benefits**.

Studies show **10–15% productivity gains** in teams that adopt coding assistants, even when you factor in some amount of rework ([bain.com](https://bain.com)). AI tools can **automate repetitive tasks**, suggest standardized solutions, and even help generate tests, freeing developers to focus on more creative or higher-level challenges. It's no coincidence that organizations that embrace AI strategically are **shipping software faster and more reliably** ([observer.com](https://observer.com)).

A leadership guideline from DX sums it up well:

> "Developers who leverage AI will outperform those who resist adoption."

([getdx.com](https://getdx.com))

In other words, there is a real **opportunity cost** in ignoring these tools – in the long run, those who learn to use them well will be able to deliver more value.

However, "embracing" AI doesn't mean doing it blindly. The key is to **balance machine efficiency with human expertise**.

No AI knows:

- the domain of your business
- the specific rules of your application
- the exact needs of your users

The people who have that knowledge are **developers and product teams**.

As one researcher put it, *"AI is only as good as the data it has access to"*. If we don't provide proper context, it will give generic and potentially useless answers ([observer.com](https://observer.com)).

It's up to the developer to provide the right instructions and, above all, **critically validate the result** before shipping anything. AI tools still make logical mistakes and do not truly understand business intent on their own – they should not be treated as infallible oracles, but as an **incredibly fast (and sometimes clumsy) pair programmer**.

Practically speaking, this means:

- reviewing each generated snippet
- writing and running tests
- adjusting details as needed

The best teams are integrating AI with good engineering practices: code reviews focused on checking whether the AI suggestion really does what it should, automated tests to catch regressions, and **clear policies** on where it's acceptable to rely on AI-generated code (for example, maybe fine for an internal script, but not for a critical module without human audit) ([bain.com](https://bain.com)).

## Conclusion: Conscious Adaptation

The arrival of AI in software development brings a mix of **excitement and anxiety**. We've seen that many developers resist it out of fear of losing space, protecting their reputation, or simply technical caution – and these concerns are valid.

On the other hand, **completely ignoring AI** can mean falling behind in productivity and professional growth. The ideal path lies in the **middle ground**: adopting these tools as **allies**, without giving up critical thinking.

Remember: **responsibility for the shipped code is still ours.** As professionals, we need to understand and **own** what we deliver to users – and no AI is going to change that.

Instead of seeing AI as a competitor, it's far more productive to view it as an **extension of our capabilities**. It helps us write code, but it cannot replace our understanding of the problem or our creativity in solving it. While AI takes care of the boilerplate, it's up to us to ensure the final product respects business rules, is secure, and is high quality.

Those who learn to use these tools with discernment will get **the best of both worlds**: speed and quality. In short, **AI can help us a lot – as long as we never give up understanding what we're delivering**.

After all, the ones who truly know the terrain (and see the bigger picture) are **humans, not machines**. With an adjusted mindset (less fear, more openness) and solid engineering practices, we can **embrace innovation without losing control of the steering wheel**.

The result is a developer who is more productive **and** more valuable – exactly because they know how to leverage new tools without compromising technical excellence.

## Sources

- Gai, Phyliss et al. *Competence Penalty Is a Barrier to the Adoption of New Technology*. SSRN, 2025. [https://papers.ssrn.com](https://papers.ssrn.com)
- Stack Overflow. *2025 Developer Survey – AI*. Stack Overflow Surveys. [https://survey.stackoverflow.co](https://survey.stackoverflow.co)
- Dey, Victor. *Google Study Shows A.I. Writes Code, But Developers Still Don't Fully Trust It*. *Observer*, Sep. 2025. [https://observer.com](https://observer.com)
- Acar, Oguz et al. *Research: The Hidden Penalty of Using AI at Work*. *Harvard Business Review*, Aug. 2025. [https://hbr.org](https://hbr.org)
- Salvador, Emilio. *6 strategies to help developers accelerate AI adoption*. GitLab Blog, Oct. 2024. [https://about.gitlab.com](https://about.gitlab.com)
- *From Pilots to Payoff: Generative AI in Software Development*. Bain & Company Tech Report, 2025. [https://bain.com](https://bain.com)
- *AI Code Generation: Best Practices for Enterprise Adoption in 2025*. DX Blog, 2025. [https://getdx.com](https://getdx.com)
- Stokel-Walker, Chris. *Trust in AI coding tools is plummeting*. LeadDev, Jul. 2025. [https://arstechnica.com](https://arstechnica.com)
