{
  "title" : "FFI with GraalVM Native Image: The Real Work of Maintaining a Library That Crosses Language Boundaries",
  "date" : "2026-01-29T00:00:00+00:00",
  "tags" : [ ],
  "url" : "https://avelino.run/ffi-graalvm-native-image-real-work",
  "content" : "Exposing a library via FFI seems simple on paper.\n\nYou compile your code to .so, export some functions with extern C, write bindings in the target language. Done, interoperability achieved.\n\nExcept that's not how it works in practice. At least not when your lib is a database written in Clojure, compiled with GraalVM native-image, that needs to manage Git and Lucene internally, and will be called from Rust in environments ranging from dev laptops to CI containers with 8MB of stack.\n\nOver the past few months, I've worked on this exact scenario with ChronDB. Every bug fixed became a lesson about how different languages coexist in the same process — and about everything that can go wrong in that coexistence.\n\nThe Scenario: Why FFI with GraalVM?\n\nChronDB is a temporal database. It uses Git as storage (each write is a commit) and Apache Lucene for full-text indexing. The core is Clojure running on the JVM.\n\nThe problem: not everyone wants (or can) run a JVM. Rust developers want a crate. Python developers want a pip package. Embedding an entire JVM in each application isn't viable.\n\nThe solution: GraalVM native-image compiles Java/Clojure bytecode to native binary. You generate a libchrondb.so that can be loaded via dlopen from any language. No ~~JVM~~, no classpath, no JAR hell.\n\n``\n┌─────────────────────────────────────┐\n│  Your application (Rust/Python/Go)  │\n├─────────────────────────────────────┤\n│         dlopen + FFI                │\n├─────────────────────────────────────┤\n│   libchrondb.so (GraalVM native)    │\n│   ├── SubstrateVM runtime           │\n│   ├── Clojure core                  │\n│   ├── JGit (Git operations)         │\n│   └── Lucene (indexing)             │\n└─────────────────────────────────────┘\n`\n\nIt works. Until it doesn't.\n\nProblem 1: StackOverflowError on put()\n\nFirst real issue that appeared. User opens the database, does get(), all good. Calls put(), crash:\n\n`\nFatal error: StackOverflowError: Enabling the yellow zone of the stack\ndid not make any stack space available.\n`\n\nWhy it happens\n\nWhen you compile Java to native-image, GraalVM embeds a minimalist runtime called SubstrateVM. Each instance of this runtime is an Isolate — an execution context with its own heap and state.\n\nWhen external code calls an FFI function (@CEntryPoint), SubstrateVM needs to:\n\n1. Register the calling thread in the runtime (so GC knows about it)\n2. Configure stack guards (yellow zone, red zone)\n3. Allocate space to capture stack trace if an exception occurs\n\nThat third point is the problem. SubstrateVM allocates space for a complete stack trace at the entry point. With Clojure + JGit + Lucene, a put() has hundreds of frames:\n\n`\nChronDB_put (entry point)\n  └→ clojure.lang.RT.var()\n      └→ chrondb.lib/lib-put\n          ├→ JGit ObjectInserter.insert()\n          │   ├→ PackInserter → SHA1.digest()\n          │   └→ RefUpdate → FileChannel.lock()\n          └→ Lucene IndexWriter.addDocument()\n              └→ DocumentsWriter → DefaultIndexingChain\n                  └→ TermsHashPerField → ByteBlockPool\n`\n\nGraalVM needs ~64MB of stack for this. Default Linux stack? 8MB. macOS varies, but usually less than needed.\n\nThe solution that doesn't work\n\nDocumentation suggests RUST_MIN_STACK=64MB. Doesn't work because:\n\n- Only affects threads created after the env var is set\n- Main thread already exists\n- Runtime threads (Tokio, Rayon) use their own defaults\n\nThe real solution\n\nEnsure that all FFI calls happen on a thread with sufficient stack. I implemented a dedicated worker thread:\n\n`rust\nimpl ChronDB {\n    pub fn open(data_path: &str, index_path: &str) -> Result<Self> {\n        let (command_tx, command_rx) = mpsc::channel();\n\n        // Worker with 64MB stack\n        let handle = thread::Builder::new()\n            .stack_size(64  1024  1024)\n            .spawn(move || {\n                let lib = load_native_lib();\n                let db_handle = unsafe { lib.open(data_path, index_path) };\n\n                // All FFI calls here\n                while let Ok(cmd) = command_rx.recv() {\n                    match cmd {\n                        Command::Put { key, value, tx } => {\n                            let result = unsafe { lib.put(db_handle, &key, &value) };\n                            tx.send(result);\n                        }\n                        Command::Shutdown => break,\n                    }\n                }\n            })?;\n\n        Ok(Self { command_tx, handle })\n    }\n}\n`\n\nUser calls db.put(), internally it becomes a message to the worker, worker executes FFI with 64MB stack, returns result via channel.\n\nTransparent to the user. Works in any environment.\n\nProblem 2: Database Won't Open After Crash\n\nSecond issue. User kills the process with Ctrl+C in the middle of an operation. Tries to reopen:\n\n`\nDatabase(OpenFailed())\n`\n\nEmpty error. Very helpful for debugging.\n\nWhy it happens\n\nJGit and Lucene use file locking to prevent concurrent access:\n\n- Git: HEAD.lock, index.lock, refs/heads/main.lock\n- Lucene: write.lock\n\nThese are advisory locks from the operating system. The kernel releases them automatically when the process terminates... normally.\n\nBut the .lock file stays on disk. And JGit checks for file existence, not the lock itself:\n\n`java\n// JGit LockFile.java\npublic boolean lock() throws IOException {\n    if (lockFile.exists()) {\n        return false;  // Assumes another process has the lock\n    }\n    // ...\n}\n`\n\n> Kill -9, OOM killer, power loss — any abnormal termination leaves the file orphaned.\n\nThe solution\n\nBefore opening, verify if locks are actually active. The technique: try to acquire the OS advisory lock. If you succeed, the file is orphaned.\n\n`clojure\n(defn orphaned-lock? [lock-file]\n  (try\n    (with-open [channel (FileChannel/open (.toPath lock-file)\n                          (into-array [StandardOpenOption/WRITE]))]\n      (if-let [lock (.tryLock channel)]\n        (do (.release lock) true)   ; Got it = orphaned\n        false))                      ; Didn't get it = in use\n    (catch OverlappingFileLockException _ false)))\n\n(defn cleanup-orphaned-locks [directory]\n  (doseq [f (find-lock-files directory)]\n    (when (orphaned-lock? f)\n      (.delete f))))\n`\n\nThe kernel maintains a lock table by inode. Process died? Kernel already cleaned up its locks. If tryLock() succeeds, no living process has the lock.\n\nAnd about the empty error? Fixed too:\n\n`clojure\n;; Before: returned nil silently\n(defn lib-open [data-path index-path]\n  (when-let [storage (create-storage data-path)]\n    (when-let [index (create-index index-path)]\n      (register-handle storage index))))\n\n;; After: propagates error with context\n(defn lib-open [data-path index-path]\n  (try\n    (let [storage (create-storage data-path)\n          index (create-index index-path)]\n      (register-handle storage index))\n    (catch Exception e\n      (log/error e lib-open failed {:data-path data-path})\n      -1)))\n`\n\nProblem 3: Data Disappears Between Processes\n\nThird issue, the most frustrating. Flow:\n\n`bash\nmyapp save-state    Saves data to ChronDB\nmyapp load-state    Data doesn't exist\n`\n\nLog from both commands:\n\n`\nResetting corrupted state database...\n`\n\nEvery invocation detected corruption and reset.\n\nWhy it happens\n\nThe initialization code always called Git.init():\n\n`clojure\n(defn create-storage [path]\n  (-> (Git/init)\n      (.setDirectory (io/file path))\n      (.call)\n      (.getRepository)))\n`\n\nGit.init() internally calls repository.create(), which rewrites HEAD to point to an empty branch. Even if the repository exists with data.\n\nEach process: init → empty HEAD → where's my data?.\n\nThe solution\n\nCheck before creating:\n\n`clojure\n(defn repository-exists? [path]\n  (let [git-dir (io/file path .git)]\n    (and (.exists git-dir)\n         (.isDirectory git-dir)\n         (.exists (io/file git-dir HEAD))\n         (.isDirectory (io/file git-dir objects)))))\n\n(defn lib-open [data-path index-path]\n  (let [storage (if (repository-exists? data-path)\n                  (open-existing-repository data-path)\n                  (create-new-repository data-path))\n        ...]\n    ...))\n`\n\nSeems obvious in retrospect. But when you're focused on making it work, it's easy to treat open and create as the same operation.\n\nProblem 4: Multiple Instances Corrupt Data\n\nFourth issue. User's code:\n\n`rust\nfn process_a() {\n    let db = ChronDB::open(/data, /index)?;\n    db.put(key, value)?;\n}\n\nfn process_b() {\n    let db = ChronDB::open(/data, /index)?;  // Same path!\n    let v = db.get(key)?;  // Sometimes works, sometimes doesn't\n}\n`\n\nWhy it happens\n\nEach open() created a new worker thread, a new GraalVM Isolate, a new JGit Repository instance and Lucene IndexWriter.\n\nOS advisory locks work between processes, not between threads in the same process accessing via different handles. Two Isolates in the same process can open the same files — and start overwriting each other's data.\n\nThe solution\n\nSingleton per path. One instance per (data_path, index_path) combination:\n\n`rust\nlazy_static! {\n    static ref REGISTRY: Mutex<HashMap<PathPair, Weak<SharedWorker>>> =\n        Mutex::new(HashMap::new());\n}\n\nimpl ChronDB {\n    pub fn open(data_path: &str, index_path: &str) -> Result<Self> {\n        let key = PathPair::new(data_path, index_path);\n        let mut registry = REGISTRY.lock().unwrap();\n\n        // Reuse if exists\n        if let Some(weak) = registry.get(&key) {\n            if let Some(worker) = weak.upgrade() {\n                return Ok(Self { worker });\n            }\n        }\n\n        // Create new\n        let worker = Arc::new(SharedWorker::new(data_path, index_path)?);\n        registry.insert(key, Arc::downgrade(&worker));\n        Ok(Self { worker })\n    }\n}\n`\n\nWeak in the registry allows the worker to be dropped when nobody uses it anymore. Next open() on the same path creates a new worker.\n\nWhat I Learned\n\nGraalVM native-image isn't compile to native\n\nThe first illusion that broke: native-image doesn't eliminate the runtime, it just embeds it. SubstrateVM is there — with GC, thread management, stack guards, safepoints. Everything the JVM does, SubstrateVM does too, just ahead-of-time instead of JIT.\n\nThis means you're not distributing pure native code. You're distributing a minimalist VM with your code pre-compiled inside. And that VM has requirements: needs heap, needs stack, needs to register threads, needs space for stack traces.\n\nWhen I read that SubstrateVM allocates space for a complete stack trace at every @CEntryPoint entry point, I understood why 8MB wasn't enough. It's not that our code is inefficient — it's that the runtime needs to be prepared to capture hundreds of frames if something goes wrong. With Clojure (which adds var resolution frames), JGit (Git operations are deep), and Lucene (indexing has many layers), the potential stack trace is enormous.\n\nThe lesson: understand the runtime you're distributing. Reading the SubstrateVM documentation about Isolates, stack guards, and thread attachment changed how I think about the problem.\n\nBindings aren't the API — they're a protection layer\n\nThe initial temptation was to make thin bindings: translate the exported C functions 1:1 to Rust idioms. ChronDB_open becomes ffi::open(), ChronDB_put becomes ffi::put(). Simple.\n\nThis approach transfers all runtime requirements to the user. Need 64MB of stack? User's problem to configure. Orphaned lock? User's problem to clean up. Multiple instances? User's problem to coordinate.\n\nWhat worked was inverting the mindset: bindings are a protection layer between the native runtime and user code.\n\n`\n┌─────────────────────────────────────────────┐\n│           User code                         │\n│  db.put(key, value)  // Simple API        │\n├─────────────────────────────────────────────┤\n│          Protection layer                   │\n│  - Worker thread with adequate stack        │\n│  - Singleton per path                       │\n│  - Orphaned lock cleanup                    │\n│  - Create vs open verification              │\n│  - Errors with context                      │\n├─────────────────────────────────────────────┤\n│     FFI calls (controlled environment)      │\n├─────────────────────────────────────────────┤\n│         libchrondb.so                       │\n└─────────────────────────────────────────────┘\n`\n\nThe user never calls FFI directly. Every call goes through the layer that ensures the conditions necessary for the runtime to work. Worker thread solves stack. Singleton solves concurrency. Cleanup solves previous crashes.\n\nDocumentation is not a solution\n\nConfigure RUST_MIN_STACK=64MB before using seems reasonable. It's not.\n\nFirst, because it doesn't work — that env var only affects threads created afterward, not the main thread. Second, because even if it worked, nobody reads runtime requirements documentation. People install the crate, call open(), and expect it to work.\n\nThird — and most importantly — because if you need to document an environment requirement, you haven't finished the work. The lib should create that environment internally.\n\nThis was the most important mindset shift: every documented requirement is a design failure. If the runtime needs 64MB of stack, the lib creates a thread with 64MB. If the runtime leaves orphaned locks, the lib cleans up before opening. If multiple instances corrupt data, the lib ensures singleton.\n\nThe work doesn't end when the function executes. It ends when the user doesn't need to know anything about the runtime to use the lib.\n\nFFI is translation between worlds with different physics\n\nRust has ownership, lifetimes, RAII. Clojure has GC, immutability, vars. GraalVM has Isolates, safepoints, compressed references. Each world has its rules about what happens when:\n\n- An object goes out of scope\n- A thread terminates\n- A process crashes\n- Memory runs out\n\nFFI isn't calling a function from another language. It's translating between worlds with different physical rules. The long handle that Clojure returns is an index in an internal table — if the GC moves the object, the handle remains valid. But if the Isolate is destroyed, all handles become invalid instantly.\n\nUnderstanding this changes how you design the boundary. You don't think which function to expose?. You think which invariants of my world do I need to protect? which invariants of the other world do I need to respect?.\n\nIn our case:\n\n- Protect from Clojure side: handles are only valid while the Isolate exists; GC can pause at safepoints; IO operations can block\n- Respect from Rust side: ownership needs to be clear; errors need to be Result, not exceptions; resources need to be released in Drop\n\nThe real cost of FFI with GraalVM\n\nAfter solving the problems, I calculated what the protection layer adds:\n\n- Latency: ~50-100μs per operation (channel send/recv + context switch to worker)\n- Memory: ~64MB for worker stack + channel overhead\n- Complexity: ~500 lines of infrastructure code in bindings\n\nFor a database where operations are IO-bound (Git write, Lucene index), 100μs is noise. 64MB is acceptable for any modern machine. 500 lines is the cost of not having to explain to every user how to configure stack size.\n\nThe tradeoff was worth it! But it's important to know it exists. Transparent FFI doesn't exist — either you pay the cost in the lib, or the user pays in bugs and configuration.\n\nLinks\n\n- ChronDB\n- PR #89 - Worker thread architecture\n- PR #93 - Fix repository-exists\n- PR #95 - Singleton pattern\n\nIf you're exposing a lib via FFI, I hope this battle documentation is useful. The problems are predictable — after you encounter them for the first time.\n\nIt wasn't as simple as I thought it would be. I thought I'd just turn ChronDB into a .so` and with a simple layer in the native language (e.g., Rust) I'd have ChronDB native in that language.",
  "type" : "blog",
  "file-path" : "content/blog/2026-01-29-ffi-graalvm-native-image-real-work.md"
}