{
  "title" : "LLM como Copiloto de Investigação: Debugando Produção com Claude Code e MCPs",
  "date" : "2026-01-27T00:00:00+00:00",
  "tags" : [ ],
  "url" : "https://avelino.run/claude-code-mcp-production-debugging",
  "content" : "Debugar em produção é um exercício de context-switching infinito.\n\nAlguém reporta um bug no Slack. Você abre o Honeycomb pra ver traces. Pula pro Sentry pra encontrar exceptions. Volta pro código pra entender o fluxo. Consulta documentação pra lembrar como aquele serviço funciona. Enquanto isso, três pessoas te marcaram em outras threads perguntando já viu isso?.\n\nO problema não é falta de dados — é excesso de contexto fragmentado. Cada ferramenta tem uma peça do quebra-cabeça, mas montar a imagem completa exige malabarismo mental constante.\n\nNos últimos meses, mudei meu workflow. Em vez de ser o humano que pula entre ferramentas, transformei o Claude Code no copiloto que faz essa navegação por mim.\n\nO Cockpit: MCPs que Conectam Tudo\n\nA mágica está nos MCPs (Model Context Protocol) — integrações que permitem ao Claude acessar ferramentas externas diretamente. Minha stack atual:\n\n``json\n{\n  mcpServers: {\n    sentry: {\n      type: http,\n      url: https://mcp.sentry.dev/mcp\n    },\n    honeycomb: {\n      type: http,\n      url: https://mcp.honeycomb.io/mcp\n    },\n    slack: {\n      command: npx,\n      args: [-y, slack-mcp-server@latest, --transport, stdio],\n      env: {\n        SLACK_MCP_XOXP_TOKEN: xoxp-...,\n        SLACK_MCP_TEAM_ID: ...\n      }\n    },\n    github: {\n      type: http,\n      url: https://api.githubcopilot.com/mcp\n    }\n  }\n}\n`\n\nCom essa configuração, o Claude pode:\n\n- Slack: Ler threads de bug reports, entender o contexto do problema\n- Honeycomb: Buscar traces, analisar latência, identificar gargalos\n- Sentry: Puxar exceptions, stack traces, frequência de erros\n- GitHub: Navegar PRs, histórico de commits, blame\n- Codebase local: Ler o código diretamente do meu terminal\n\nO Claude não vê cada ferramenta isoladamente — ele cruza informações. E esse é o diferencial.\n\nUm Caso Real: Queries Travando o Sistema\n\nSexta-feira, 17h. O pior horário. Thread no Slack: Sistema inteiro lento, várias telas não carregam, timeout em tudo.\n\nEsse tipo de report é o mais difícil. Tudo lento pode ser mil coisas: CPU, memória, rede, banco, serviço externo. Sem um ponto de entrada específico, a investigação vira caça ao tesouro.\n\nAntes do workflow com MCPs, eu:\n\n1. Abriria Honeycomb, filtraria por latência alta no intervalo de tempo\n2. Encontraria traces lentos, clicaria em cada um tentando achar padrão\n3. Notaria spans de database demorados, anotaria mentalmente\n4. Abriria Sentry, buscaria exceptions no mesmo período\n5. Tentaria correlacionar manualmente trace ID com exception\n6. Voltaria pro código, grep pelo nome do serviço suspeito\n7. Leria implementação, formularia hipótese\n8. Voltaria às ferramentas pra validar\n\nTempo médio: depende. Às vezes 30 minutos, às vezes 2 horas. O problema não é cada passo — é manter o contexto mental entre eles.\n\nPedi pro Claude:\n\n> Sistema com lentidão generalizada nos últimos 20min. Busca no Honeycomb traces com latência > 3s, agrupa por serviço pra identificar qual está degradado. Se for banco, procura por queries lentas ou locks.\n\nO Claude encontrou o padrão:\n\n1. Honeycomb: 80% dos traces lentos tinham spans de database com duração > 5s\n2. Drill-down: As queries lentas eram todas no mesmo serviço — relatórios\n3. Sentry: Várias exceptions de LockWaitTimeout no mesmo período\n4. Código local: Encontrou a query problemática no endpoint de exportação de relatório\n\nO diagnóstico:\n\n> Causa raiz: O endpoint de exportação de relatório executa uma query analítica pesada (SELECT ... GROUP BY ... ORDER BY) sem paginação. Quando dois usuários exportam simultaneamente, a segunda query fica esperando lock da primeira. A query leva ~45s para completar, e o lock_wait_timeout é 30s — resultado: timeout em cascata.\n>\n> Por que afeta tudo: A tabela principal (orders) é usada por quase todos os endpoints. Enquanto a query analítica segura lock de leitura em milhões de rows, outras operações que precisam de write lock ficam enfileiradas. Efeito dominó.\n>\n> Solução proposta:\n>\n> 1. Mover queries analíticas para réplica de leitura (read replica)\n> 2. Adicionar índice composto para a query de relatório (created_at, status, user_id`)\n> 3. Implementar paginação com cursor no export — processar em chunks de 10k rows\n> 4. Considerar materializar esse relatório em tabela separada (atualização async)\n\nEsse caso ilustra algo importante: o Claude não só achou o problema, ele explicou por que um endpoint de relatório afetava o sistema inteiro. Essa conexão entre lock de leitura → block de escrita → efeito dominó é exatamente o tipo de raciocínio que exige manter múltiplos conceitos na cabeça simultaneamente.\n\nA Estrutura do Pedido que Funciona\n\nDepois de semanas experimentando, identifiquei o padrão de prompt que funciona:\n\nO que funciona\n\n1. Contexto específico: URL, horário, user ID, qualquer identificador concreto\n2. Escopo claro: últimos 30min, serviço X, fluxo Y\n3. Pedido de diagnóstico, não código: me diz a causa > arruma isso\n4. Qualificador de solução: definitiva, não paliativa\n\nO que não funciona\n\n- ❌ Tem um bug no checkout, descobre o que é (muito vago)\n- ❌ Escreve o fix pra esse problema (pula a investigação)\n- ❌ Olha todos os erros da última semana (escopo impossível)\n\nA regra de ouro: nunca peço código, peço diagnóstico + proposta de solução.\n\nPor quê? Porque código é a parte fácil. A parte difícil é entender o problema. Quando o LLM pula direto pra código, ele frequentemente resolve o sintoma, não a causa. Quando ele precisa primeiro diagnosticar, ele é forçado a construir modelo mental do sistema — e eu posso validar esse modelo antes de qualquer implementação.\n\nQuando Não Funciona\n\nSeria desonesto dizer que funciona sempre. O principal limitador é contexto muito distribuído.\n\nQuando o bug envolve 5+ serviços, cada um com suas próprias traces e exceptions, o Claude começa a perder o fio. Ele consegue puxar dados de cada fonte, mas a correlação entre elas fica superficial. Nesses casos, uso o workflow híbrido: deixo ele fazer a primeira triagem (eliminar hipóteses óbvias), depois assumo a investigação manual nas partes que exigem correlação complexa.\n\nTambém não funciona bem para:\n\n- Race conditions: O modelo não consegue ver timing de execução\n- Problemas de estado: Se o bug depende de sequência específica de eventos\n- Dados que não posso compartilhar: Às vezes o contexto necessário é sensível demais\n\nO Modelo Mental Certo\n\nO erro comum é tratar LLM como desenvolvedor automático. Não é. É copiloto de investigação.\n\nA analogia que uso: copiloto de avião. O piloto (você) decide destino, rota, quando decolar. O copiloto (LLM) monitora instrumentos, faz checklist, alerta sobre anomalias. Você não pede pro copiloto pilota o avião — você pede checa pressão do óleo ou confirma altitude com torre.\n\nCom debugging é igual:\n\n- ✅ Busca traces com latência > 5s nesse serviço\n- ✅ Cruza esse exception com o código e me diz onde origina\n- ✅ Lista hipóteses do que pode causar esse comportamento\n- ❌ Arruma o bug\n\nSetup Mínimo pra Testar\n\nSe você quer experimentar esse workflow, o setup mínimo:\n\n1. Claude Code CLI instalado\n2. MCP do Honeycomb (ou sua ferramenta de observabilidade)\n3. MCP do Sentry (ou equivalente)\n4. Acesso ao codebase no mesmo terminal\n\nNão precisa de todos os MCPs de uma vez. Comece com observabilidade + exceptions. Adicione Slack depois se fizer sentido pro seu fluxo.\n\nO Que Muda na Prática\n\nO ganho real não é resolver bug em 5 minutos que levaria 2 horas. Às vezes leva o mesmo tempo. O ganho é qualidade da investigação.\n\nQuando eu pulava entre ferramentas, frequentemente parava na primeira hipótese plausível. Ah, queries lentas? Deve ser índice faltando. Cria um índice. Solução paliativa. Bug volta em duas semanas.\n\nCom o LLM fazendo a correlação, ele naturalmente continua investigando além da primeira hipótese. Query lenta, sim, mas por quê? Tem lock envolvido? Por que essa query específica trava as outras? Qual o efeito cascata?\n\nA profundidade da investigação aumenta porque o custo cognitivo de manter contexto diminui.\n\nConclusão\n\nLLMs estão mudando como escrevemos código. Mas talvez o impacto maior seja em como investigamos sistemas.\n\nDebugging em produção é fundamentalmente um problema de integração de contexto. Cada ferramenta tem uma peça. O trabalho humano era ser o hub que conecta tudo. Agora esse hub pode ser automatizado — não a decisão final, mas a coleta e correlação.\n\nSe você está usando LLM só pra autocompletar código, está subutilizando. Experimente como copiloto de investigação. Configure uns MCPs. Jogue um bug report e peça diagnóstico, não fix.\n\nOs links dos MCPs que uso:\n\n- Honeycomb MCP\n- Sentry MCP\n- Slack MCP Server\n- GitHub MCP\n\nA stack que montei funciona pra mim. A sua vai ser diferente. O princípio é o mesmo: conecte as fontes de contexto, peça diagnóstico, valide antes de implementar.",
  "type" : "blog",
  "file-path" : "content/blog/2026-01-27-claude-code-mcp-production-debugging.md"
}